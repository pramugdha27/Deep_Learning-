{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optimiser\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import torch.nn.functional as Func\n",
        "import pandas as pd\n",
        "import time"
      ],
      "metadata": {
        "id": "sifbNFF5tyqa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgMW8MBPt3ZE",
        "outputId": "8b180896-083f-43ae-ccfe-1b6f0d46dcc3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POY62YyhtsqT",
        "outputId": "b11cc4fa-8046-4ed3-a7da-1cd9de7a1a36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch Count [1/25], Training Loss: 1.7587, Training Accuracy: 35.26%, Validation Loss: 1.4995, Validation Accuracy: 44.59%\n",
            "Epoch Count [2/25], Training Loss: 1.3899, Training Accuracy: 49.71%, Validation Loss: 1.2512, Validation Accuracy: 54.43%\n",
            "Epoch Count [3/25], Training Loss: 1.1794, Training Accuracy: 58.29%, Validation Loss: 1.1752, Validation Accuracy: 59.44%\n",
            "Epoch Count [4/25], Training Loss: 1.0637, Training Accuracy: 62.77%, Validation Loss: 1.0100, Validation Accuracy: 64.37%\n",
            "Epoch Count [5/25], Training Loss: 0.9884, Training Accuracy: 65.29%, Validation Loss: 1.0118, Validation Accuracy: 64.67%\n",
            "Epoch Count [6/25], Training Loss: 0.9468, Training Accuracy: 66.71%, Validation Loss: 1.0399, Validation Accuracy: 64.05%\n",
            "Epoch Count [7/25], Training Loss: 0.9045, Training Accuracy: 68.38%, Validation Loss: 0.9268, Validation Accuracy: 67.16%\n",
            "Epoch Count [8/25], Training Loss: 0.8750, Training Accuracy: 69.68%, Validation Loss: 0.9715, Validation Accuracy: 66.90%\n",
            "Epoch Count [9/25], Training Loss: 0.8470, Training Accuracy: 70.26%, Validation Loss: 0.8583, Validation Accuracy: 71.08%\n",
            "Epoch Count [10/25], Training Loss: 0.8250, Training Accuracy: 71.08%, Validation Loss: 0.8207, Validation Accuracy: 71.66%\n",
            "Epoch Count [11/25], Training Loss: 0.8054, Training Accuracy: 72.14%, Validation Loss: 0.8793, Validation Accuracy: 69.75%\n",
            "Epoch Count [12/25], Training Loss: 0.7898, Training Accuracy: 72.58%, Validation Loss: 0.8274, Validation Accuracy: 71.70%\n",
            "Epoch Count [13/25], Training Loss: 0.7747, Training Accuracy: 73.30%, Validation Loss: 0.7708, Validation Accuracy: 73.12%\n",
            "Epoch Count [14/25], Training Loss: 0.7632, Training Accuracy: 73.43%, Validation Loss: 0.7877, Validation Accuracy: 72.85%\n",
            "Epoch Count [15/25], Training Loss: 0.7530, Training Accuracy: 73.77%, Validation Loss: 0.8022, Validation Accuracy: 72.44%\n",
            "Epoch Count [16/25], Training Loss: 0.7432, Training Accuracy: 74.29%, Validation Loss: 0.7602, Validation Accuracy: 74.10%\n",
            "Epoch Count [17/25], Training Loss: 0.7246, Training Accuracy: 74.64%, Validation Loss: 0.7445, Validation Accuracy: 74.52%\n",
            "Epoch Count [18/25], Training Loss: 0.7109, Training Accuracy: 75.16%, Validation Loss: 0.7327, Validation Accuracy: 74.86%\n",
            "Epoch Count [19/25], Training Loss: 0.7084, Training Accuracy: 75.26%, Validation Loss: 0.7318, Validation Accuracy: 74.47%\n",
            "Epoch Count [20/25], Training Loss: 0.6998, Training Accuracy: 75.66%, Validation Loss: 0.7191, Validation Accuracy: 75.46%\n",
            "Epoch Count [21/25], Training Loss: 0.6964, Training Accuracy: 75.73%, Validation Loss: 0.7481, Validation Accuracy: 73.68%\n",
            "Epoch Count [22/25], Training Loss: 0.6834, Training Accuracy: 76.53%, Validation Loss: 0.6927, Validation Accuracy: 76.10%\n",
            "Epoch Count [23/25], Training Loss: 0.6790, Training Accuracy: 76.53%, Validation Loss: 0.6906, Validation Accuracy: 76.56%\n",
            "Epoch Count [24/25], Training Loss: 0.6741, Training Accuracy: 76.69%, Validation Loss: 0.7412, Validation Accuracy: 74.76%\n",
            "Epoch Count [25/25], Training Loss: 0.6658, Training Accuracy: 77.03%, Validation Loss: 0.7359, Validation Accuracy: 74.96%\n",
            "The training time is : 772.18 seconds\n"
          ]
        }
      ],
      "source": [
        "# Defining the transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),# flipping the image/mirroring it\n",
        "    transforms.RandomCrop(32, padding=4), # cropping randomly\n",
        "    transforms.ToTensor(), # convert image to tensor\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # normalises to mean:0 and std:1\n",
        "])\n",
        "\n",
        "\n",
        "# Loading the CIFAR-10 dataset / Getting cifar-10 (test/train) and preprocessing using transform\n",
        "batchsize = 32\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_set, batch_size=batchsize, shuffle=True, num_workers=2)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_set, batch_size=batchsize, shuffle=False, num_workers=2)\n",
        "\n",
        "class BasicCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BasicCNN, self).__init__()\n",
        "\n",
        "        self.conv_1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn_1 = nn.BatchNorm2d(12)\n",
        "        self.conv_2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn_2 = nn.BatchNorm2d(24)\n",
        "        self.pool_1 = nn.MaxPool2d(2,2)\n",
        "        self.conv_4 = nn.Conv2d(in_channels= 24, out_channels=48, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn_4 = nn.BatchNorm2d(48)\n",
        "        self.pool_2 = nn.MaxPool2d(2,2)\n",
        "        self.conv_5 = nn.Conv2d(in_channels= 48, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn_5 = nn.BatchNorm2d(24)\n",
        "        self.conv_6 = nn.Conv2d(in_channels= 24, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn_6 = nn.BatchNorm2d(12)\n",
        "\n",
        "        self.fL_1 = nn.Linear(12*10*10, 10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        output_m = Func.relu(self.bn_1(self.conv_1(x)))\n",
        "        output_m = Func.relu(self.bn_2(self.conv_2(output_m)))\n",
        "        output_m = self.pool_1(output_m)\n",
        "        output_m = Func.relu(self.bn_4(self.conv_4(output_m)))\n",
        "        output_m = Func.relu(self.bn_5(self.conv_5(output_m)))\n",
        "\n",
        "        output_m = Func.relu(self.bn_6(self.conv_6(output_m)))\n",
        "        output_m = output_m.view(-1, 12*10*10)\n",
        "        #output_m = output_m.view(-1, 24*10*10)\n",
        "\n",
        "        output_m = self.fL_1(output_m)\n",
        "\n",
        "        return output_m\n",
        "\n",
        "basic_cnn_model = BasicCNN()\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optimiser.Adam(basic_cnn_model.parameters(), lr=0.01)\n",
        "\n",
        "def training_model(model, optimizer, epochs=25):\n",
        "    training_accuracy_history = []\n",
        "    validation_accuracy_history = []\n",
        "    training_loss_history = []\n",
        "    validation_loss_history = []\n",
        "\n",
        "    device_test = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device_test)\n",
        "\n",
        "    starting_time = time.time()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        run_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for data in train_loader:\n",
        "            input_train, label_train = data\n",
        "            input_train, label_train = input_train.to(device_test), label_train.to(device_test)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output_train = model(input_train)\n",
        "            loss_train = loss_func(output_train, label_train)\n",
        "            loss_train.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            run_loss =  run_loss + loss_train.item()\n",
        "            torch_val, predicted_train = torch.max(output_train.data, 1)\n",
        "            total_train = total_train + label_train.size(0)\n",
        "            correct_train = correct_train + (predicted_train == label_train).sum().item()\n",
        "\n",
        "        training_loss = run_loss / len(train_loader)\n",
        "        training_accuracy = 100 * correct_train / total_train\n",
        "        training_loss_history.append(training_loss)\n",
        "        training_accuracy_history.append(training_accuracy)\n",
        "\n",
        "        # Doing the Validating\n",
        "        model.eval()\n",
        "        vali_correct = 0\n",
        "        vali_total = 0\n",
        "        validation_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in test_loader:\n",
        "                vali_input, vali_label = data\n",
        "                vali_input, vali_label = vali_input.to(device_test), vali_label.to(device_test)\n",
        "                vali_output = model(vali_input)\n",
        "                vali_loss = loss_func(vali_output, vali_label)\n",
        "                validation_loss = validation_loss + vali_loss.item()\n",
        "                torch_val, predicted_vali = torch.max(vali_output.data, 1)\n",
        "                vali_total = vali_total + vali_label.size(0)\n",
        "                vali_correct = vali_correct +  (predicted_vali == vali_label).sum().item()\n",
        "\n",
        "        validation_loss = validation_loss / len(test_loader)\n",
        "        vali_accuracy = 100 * vali_correct / vali_total\n",
        "        validation_loss_history.append(validation_loss)\n",
        "        validation_accuracy_history.append(vali_accuracy)\n",
        "\n",
        "        print(f'Epoch Count [{ep + 1}/{epochs}], '\n",
        "              f'Training Loss: {training_loss:.4f}, '\n",
        "              f'Training Accuracy: {training_accuracy:.2f}%, '\n",
        "              f'Validation Loss: {validation_loss:.4f}, '\n",
        "              f'Validation Accuracy: {vali_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "    ending_time = time.time()  # Record the end time\n",
        "    total_elapsed_time = ending_time - starting_time  # Calculate elapsed time\n",
        "\n",
        "    print(f'The training time is : {total_elapsed_time:.2f} seconds')  # Print the training time\n",
        "\n",
        "    return training_accuracy_history, training_loss_history, validation_accuracy_history, validation_loss_history\n",
        "\n",
        "# Training the basic CNN model\n",
        "training_accuracy_history, training_loss_history, validation_accuracy_history, validation_loss_history = training_model(basic_cnn_model, optimizer)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uD_7E-s7twEz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}