{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H57m3h6tyTO4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optimiser\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import torch.nn.functional as Func\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "vQMAIRbuyn6X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6bd0d8f-15fe-4db1-e13e-131251e8077a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),# flipping the image/mirroring it\n",
        "    transforms.RandomCrop(32, padding=4), # cropping randomly\n",
        "    transforms.ToTensor(), # convert image to tensor\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # normalises to mean:0 and std:1\n",
        "])\n",
        "\n",
        "\n",
        "# Loading the CIFAR-10 dataset / Getting cifar-10 (test/train) and preprocessing using transform\n",
        "batchsize = 32\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_set, batch_size=batchsize, shuffle=True, num_workers=2)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_set, batch_size=batchsize, shuffle=False, num_workers=2)\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNet, self).__init__()\n",
        "        # Define your DenseNet architecture here\n",
        "        self.model = torchvision.models.densenet121(pretrained=False, num_classes=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "densenet121_model = DenseNet()\n",
        "\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optimiser.Adam(densenet121_model.parameters(), lr=0.01)\n",
        "\n",
        "def training_model(model, optimizer, epochs=25):\n",
        "    training_accuracy_history = []\n",
        "    validation_accuracy_history = []\n",
        "    training_loss_history = []\n",
        "    validation_loss_history = []\n",
        "\n",
        "    device_test = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device_test)\n",
        "\n",
        "    starting_time = time.time()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        run_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for data in train_loader:\n",
        "            input_train, label_train = data\n",
        "            input_train, label_train = input_train.to(device_test), label_train.to(device_test)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output_train = model(input_train)\n",
        "            loss_train = loss_func(output_train, label_train)\n",
        "            loss_train.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            run_loss =  run_loss + loss_train.item()\n",
        "            torch_val, predicted_train = torch.max(output_train.data, 1)\n",
        "            total_train = total_train + label_train.size(0)\n",
        "            correct_train = correct_train + (predicted_train == label_train).sum().item()\n",
        "\n",
        "        training_loss = run_loss / len(train_loader)\n",
        "        training_accuracy = 100 * correct_train / total_train\n",
        "        training_loss_history.append(training_loss)\n",
        "        training_accuracy_history.append(training_accuracy)\n",
        "\n",
        "       # Doing the Validating\n",
        "        model.eval()\n",
        "        vali_correct = 0\n",
        "        vali_total = 0\n",
        "        validation_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in test_loader:\n",
        "                vali_input, vali_label = data\n",
        "                vali_input, vali_label = vali_input.to(device_test), vali_label.to(device_test)\n",
        "                vali_output = model(vali_input)\n",
        "                vali_loss = loss_func(vali_output, vali_label)\n",
        "                validation_loss = validation_loss + vali_loss.item()\n",
        "                torch_val, predicted_vali = torch.max(vali_output.data, 1)\n",
        "                vali_total = vali_total + vali_label.size(0)\n",
        "                vali_correct = vali_correct +  (predicted_vali == vali_label).sum().item()\n",
        "\n",
        "        validation_loss = validation_loss / len(test_loader)\n",
        "        vali_accuracy = 100 * vali_correct / vali_total\n",
        "        validation_loss_history.append(validation_loss)\n",
        "        validation_accuracy_history.append(vali_accuracy)\n",
        "\n",
        "        print(f'Epoch Count [{ep + 1}/{epochs}], '\n",
        "              f'Training Loss: {training_loss:.4f}, '\n",
        "              f'Training Accuracy: {training_accuracy:.2f}%, '\n",
        "              f'Validation Loss: {validation_loss:.4f}, '\n",
        "              f'Validation Accuracy: {vali_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "    ending_time = time.time()  # Record the end time\n",
        "    total_elapsed_time = ending_time - starting_time  # Calculate elapsed time\n",
        "\n",
        "    print(f'The training time is : {total_elapsed_time:.2f} seconds')  # Print the training time\n",
        "\n",
        "    return training_accuracy_history, training_loss_history, validation_accuracy_history, validation_loss_history\n",
        "\n",
        "\n",
        "# Training the basic CNN model\n",
        "training_accuracy_history, training_loss_history, validation_accuracy_history, validation_loss_history = training_model(densenet121_model, optimizer)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGIm1AXTyptY",
        "outputId": "e1ebb165-65c6-49de-853f-d1eb03e5f19c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 46362400.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Count [1/25], Training Loss: 1.8723, Training Accuracy: 31.61%, Validation Loss: 1.5097, Validation Accuracy: 43.68%\n",
            "Epoch Count [2/25], Training Loss: 1.4209, Training Accuracy: 48.33%, Validation Loss: 1.3603, Validation Accuracy: 52.14%\n",
            "Epoch Count [3/25], Training Loss: 1.2094, Training Accuracy: 56.66%, Validation Loss: 1.1225, Validation Accuracy: 59.11%\n",
            "Epoch Count [4/25], Training Loss: 1.0587, Training Accuracy: 62.21%, Validation Loss: 1.0256, Validation Accuracy: 63.34%\n",
            "Epoch Count [5/25], Training Loss: 0.9504, Training Accuracy: 66.62%, Validation Loss: 0.8415, Validation Accuracy: 70.06%\n",
            "Epoch Count [6/25], Training Loss: 0.8666, Training Accuracy: 69.83%, Validation Loss: 0.8011, Validation Accuracy: 72.00%\n",
            "Epoch Count [7/25], Training Loss: 0.8012, Training Accuracy: 72.15%, Validation Loss: 0.7826, Validation Accuracy: 73.07%\n",
            "Epoch Count [8/25], Training Loss: 0.7548, Training Accuracy: 73.77%, Validation Loss: 0.7284, Validation Accuracy: 75.20%\n",
            "Epoch Count [9/25], Training Loss: 0.7141, Training Accuracy: 75.16%, Validation Loss: 0.7040, Validation Accuracy: 75.90%\n",
            "Epoch Count [10/25], Training Loss: 0.6802, Training Accuracy: 76.48%, Validation Loss: 0.6522, Validation Accuracy: 77.71%\n",
            "Epoch Count [11/25], Training Loss: 0.6467, Training Accuracy: 77.75%, Validation Loss: 0.6481, Validation Accuracy: 77.56%\n",
            "Epoch Count [12/25], Training Loss: 0.6212, Training Accuracy: 78.67%, Validation Loss: 0.6127, Validation Accuracy: 78.59%\n",
            "Epoch Count [13/25], Training Loss: 0.5947, Training Accuracy: 79.63%, Validation Loss: 0.6158, Validation Accuracy: 78.22%\n",
            "Epoch Count [14/25], Training Loss: 0.5762, Training Accuracy: 80.17%, Validation Loss: 0.5987, Validation Accuracy: 79.72%\n",
            "Epoch Count [15/25], Training Loss: 0.5587, Training Accuracy: 80.70%, Validation Loss: 0.5899, Validation Accuracy: 79.83%\n",
            "Epoch Count [16/25], Training Loss: 0.5358, Training Accuracy: 81.51%, Validation Loss: 0.6076, Validation Accuracy: 79.29%\n",
            "Epoch Count [17/25], Training Loss: 0.5183, Training Accuracy: 82.11%, Validation Loss: 0.5689, Validation Accuracy: 80.80%\n",
            "Epoch Count [18/25], Training Loss: 0.5023, Training Accuracy: 82.70%, Validation Loss: 0.5368, Validation Accuracy: 81.77%\n",
            "Epoch Count [19/25], Training Loss: 0.4900, Training Accuracy: 82.95%, Validation Loss: 0.5485, Validation Accuracy: 81.30%\n",
            "Epoch Count [20/25], Training Loss: 0.4794, Training Accuracy: 83.45%, Validation Loss: 0.5452, Validation Accuracy: 81.27%\n",
            "Epoch Count [21/25], Training Loss: 0.4618, Training Accuracy: 84.02%, Validation Loss: 0.5584, Validation Accuracy: 80.75%\n",
            "Epoch Count [22/25], Training Loss: 0.4491, Training Accuracy: 84.34%, Validation Loss: 0.4948, Validation Accuracy: 82.79%\n",
            "Epoch Count [23/25], Training Loss: 0.4419, Training Accuracy: 84.62%, Validation Loss: 0.5121, Validation Accuracy: 82.61%\n",
            "Epoch Count [24/25], Training Loss: 0.4246, Training Accuracy: 85.27%, Validation Loss: 0.5419, Validation Accuracy: 81.63%\n",
            "Epoch Count [25/25], Training Loss: 0.4182, Training Accuracy: 85.55%, Validation Loss: 0.5291, Validation Accuracy: 81.74%\n",
            "The training time is : 3209.41 seconds\n"
          ]
        }
      ]
    }
  ]
}